{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from data_partitioner_jsonl import *\n",
    "from lisa_tagging import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'corpus.jsonl'\n",
    "dataset = \"blogcorpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# input_path = os.path.join(f'/shared/3/projects/hiatus/idiolect/data/full_pilot/{dataset}', file_name)\n",
    "# partition_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/{dataset}', file_name.split('.')[0]) \n",
    "# tag_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-{dataset}', file_name.split('.')[0])\n",
    "# output_dir= f'/shared/3/projects/hiatus/tagged_data/lisa-{dataset}'\n",
    "input_path = os.path.join(f'/shared/3/projects/hiatus/idiolect/data/full_pilot/{dataset}', file_name)\n",
    "partition_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/{dataset}', file_name.split('.')[0]) \n",
    "tag_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-{dataset}', file_name.split('.')[0])\n",
    "output_dir= f'/shared/3/projects/hiatus/tagged_data/lisa-{dataset}'\n",
    "\n",
    "\n",
    "# ensures all directories will exist\n",
    "for directory in [partition_dir, tag_dir, output_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the file into 50 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "partition_file(input_path, partition_dir, chunks=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag each partition with 1 CPU \n",
    "\n",
    "Set nice value low so we don't hog the server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 workers for tagging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'EncT5Tokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'EncT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'tokenization_enc_t5.EncT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using the default legacy behaviour of the <class 'tokenization_enc_t5.EncT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging file /shared/3/projects/hiatus/tagged_data/lisa_partitions/blogcorpus/corpus/partition-1.jsonl with worker 622794\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/lisa_partitions/blogcorpus/corpus/partition-2.jsonl with worker 622793\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-1-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-1-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-1-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-1-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-1-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n",
      "Appending chunk to /shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-blogcorpus/corpus/partition-2-tagged.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import logging\n",
    "original_level = logging.getLogger().getEffectiveLevel()\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "tag_partitions(\n",
    "               input_directory=partition_dir,\n",
    "               output_directory=tag_dir,\n",
    "               num_workers=2,\n",
    "#                default_niceness=0\n",
    "               )\n",
    "logging.getLogger().setLevel(original_level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the partitioned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_tagged_files(input_directory=tag_dir,\n",
    "#                   output_file=os.path.join(output_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the temp directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_partitioned_files(partition_dir)\n",
    "# delete_partitioned_files(tag_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
