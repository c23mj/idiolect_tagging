{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from data_partitioner_jsonl import *\n",
    "from lisa_tagging import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'corpus.jsonl'\n",
    "dataset = \"blogcorpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 3 µs, total: 7 µs\n",
      "Wall time: 17.2 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# input_path = os.path.join(f'/shared/3/projects/hiatus/idiolect/data/full_pilot/{dataset}', file_name)\n",
    "# partition_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/{dataset}', file_name.split('.')[0]) \n",
    "# tag_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-{dataset}', file_name.split('.')[0])\n",
    "# output_dir= f'/shared/3/projects/hiatus/tagged_data/lisa-{dataset}'\n",
    "input_path = os.path.join(f'/shared/3/projects/hiatus/idiolect/data/full_pilot/{dataset}', file_name)\n",
    "partition_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/{dataset}', file_name.split('.')[0]) \n",
    "tag_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-{dataset}', file_name.split('.')[0])\n",
    "output_dir= f'/shared/3/projects/hiatus/tagged_data/lisa-{dataset}'\n",
    "\n",
    "\n",
    "# ensures all directories will exist\n",
    "for directory in [partition_dir, tag_dir, output_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the file into 50 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:158763 lines in file\n",
      "INFO:root:Chunk size: 79382 lines\n",
      "INFO:root:Saving /shared/3/projects/hiatus/tagged_data/lisa_partitions/blogcorpus/corpus/partition-1.jsonl\n",
      "INFO:root:Saving /shared/3/projects/hiatus/tagged_data/lisa_partitions/blogcorpus/corpus/partition-2.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.92 s, sys: 1.6 s, total: 9.52 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "partition_file(input_path, partition_dir, chunks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_enc_t5 import EncT5Tokenizer\n",
    "from modeling_enc_t5 import EncT5ForSequenceClassification\n",
    "import os \n",
    "import json \n",
    "\n",
    "tokenizer = EncT5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = EncT5ForSequenceClassification.from_pretrained(\n",
    "    \"/shared/3/projects/hiatus/idiolect/models/stylegenome_lisa_sfam/lisa_checkpoint\",\n",
    "    num_labels=768, problem_type=\"regression\"\n",
    ")\n",
    "\n",
    "def tag_lisa(obj):\n",
    "    try:\n",
    "        tokenized = tokenizer(\n",
    "            [obj['fullText']],\n",
    "            truncation=True, max_length=512, padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        print(f\"Tokenized input for object: {obj['documentID']}\")\n",
    "        prediction = model.forward(**tokenized)[0][0].cpu().detach().float()\n",
    "        print(f\"Prediction for object {obj['documentID']}: {prediction}\")\n",
    "        vector = torch.clamp(prediction, min=0.0, max=1.0).numpy().tolist()\n",
    "        if 'encodings' in obj:\n",
    "            del obj['encodings']\n",
    "        obj['lisa_vector'] = vector\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing object {obj['documentID']}: {e}\")\n",
    "    return obj\n",
    "\n",
    "def tag_partition(input_file, output_file):\n",
    "    print(f\"Tagging file {input_file} with worker {os.getpid()}\")\n",
    "    tagged_objects = []\n",
    "\n",
    "    try:\n",
    "        with open(input_file, 'r') as reader:\n",
    "            for line in reader:\n",
    "                obj = json.loads(line.strip())\n",
    "                tagged_object = tag_lisa(obj)\n",
    "                tagged_objects.append(tagged_object)\n",
    "                print(f\"Tagged object {obj['documentID']} for worker {os.getpid()}. Count: {len(tagged_objects)}\")\n",
    "\n",
    "                if len(tagged_objects) % 10 == 0:\n",
    "                    print(f\"Appending chunk of 10 objects to {output_file}\")\n",
    "                    append_chunk(output_file, tagged_objects)\n",
    "                    tagged_objects = []\n",
    "\n",
    "        if tagged_objects:\n",
    "            append_chunk(output_file, tagged_objects)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tagging partition {input_file}: {e}\")\n",
    "\n",
    "tag_partition('/shared/3/projects/hiatus/tagged_data/lisa_partitions/blogcorpus/corpus/partition-1.jsonl', 'test_tagged.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag each partition with 1 CPU \n",
    "\n",
    "Set nice value low so we don't hog the server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting CPU affinity to use 2 CPUs\n",
      "Starting multiprocessing pool...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'EncT5Tokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'EncT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'tokenization_enc_t5.EncT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using the default legacy behaviour of the <class 'tokenization_enc_t5.EncT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 701992 initialized\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/lisa_partitions/blogcorpus/corpus/partition-1.jsonl with worker 701992\n",
      "Worker 701993 initialized\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/lisa_partitions/blogcorpus/corpus/partition-2.jsonl with worker 701993Tokenized input for object: 39cc6cd5-f27a-44b7-accb-9c504fe8ff71\n",
      "\n",
      "Tokenized input for object: 01f97b8b-a28f-4bb7-b303-dab4315893d0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m~/lisa-tagging/corpora_tagging/lisa_tagging.py:27\u001b[0m, in \u001b[0;36mtag_partitions\u001b[0;34m(input_directory, output_directory, num_workers)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting multiprocessing pool...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(num_workers, initializer\u001b[38;5;241m=\u001b[39minitialize_worker, initargs\u001b[38;5;241m=\u001b[39m()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 27\u001b[0m     pool\u001b[38;5;241m.\u001b[39mstarmap(tag_partition, process_args)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished multiprocessing pool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, starmapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import logging\n",
    "original_level = logging.getLogger().getEffectiveLevel()\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "tag_partitions(\n",
    "               input_directory=partition_dir,\n",
    "               output_directory=tag_dir,\n",
    "               num_workers=2,\n",
    "#                default_niceness=0\n",
    "               )\n",
    "logging.getLogger().setLevel(original_level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the partitioned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_tagged_files(input_directory=tag_dir,\n",
    "#                   output_file=os.path.join(output_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the temp directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_partitioned_files(partition_dir)\n",
    "# delete_partitioned_files(tag_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
