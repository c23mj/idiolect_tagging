{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from data_partitioner_jsonl import *\n",
    "from lisa_tagging import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'corpus.jsonl'\n",
    "dataset = \"blogcorpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# input_path = os.path.join(f'/shared/3/projects/hiatus/idiolect/data/full_pilot/{dataset}', file_name)\n",
    "# partition_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/{dataset}', file_name.split('.')[0]) \n",
    "# tag_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-{dataset}', file_name.split('.')[0])\n",
    "# output_dir= f'/shared/3/projects/hiatus/tagged_data/lisa-{dataset}'\n",
    "input_path = os.path.join(f'/shared/3/projects/hiatus/idiolect/data/full_pilot/{dataset}', file_name)\n",
    "partition_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/{dataset}', file_name.split('.')[0]) \n",
    "tag_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-{dataset}', file_name.split('.')[0])\n",
    "output_dir= f'/shared/3/projects/hiatus/tagged_data/lisa-{dataset}'\n",
    "\n",
    "\n",
    "# ensures all directories will exist\n",
    "for directory in [partition_dir, tag_dir, output_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the file into 50 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# partition_file(input_path, partition_dir, chunks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenization_enc_t5 import EncT5Tokenizer\n",
    "# from modeling_enc_t5 import EncT5ForSequenceClassification\n",
    "# import os\n",
    "# import json\n",
    "# import torch\n",
    "# import time\n",
    "\n",
    "# tokenizer = EncT5Tokenizer.from_pretrained(\"t5-base\")\n",
    "# model = EncT5ForSequenceClassification.from_pretrained(\n",
    "#     \"/shared/3/projects/hiatus/idiolect/models/stylegenome_lisa_sfam/lisa_checkpoint\",\n",
    "#     num_labels=768, problem_type=\"regression\"\n",
    "# )\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "# def tag_lisa(obj):\n",
    "#     global tokenizer, model\n",
    "#     start_time = time.time()  # Start timing\n",
    "#     try:\n",
    "#         tokenized = tokenizer(\n",
    "#             [obj['fullText']],\n",
    "#             truncation=True, max_length=512, padding=True, return_tensors=\"pt\"\n",
    "#         )\n",
    "#         with torch.no_grad():\n",
    "#             prediction = model.forward(**tokenized)[0][0].cpu().float()\n",
    "#         vector = torch.clamp(prediction, min=0.0, max=1.0).numpy().tolist()\n",
    "#         if 'encodings' in obj:\n",
    "#             del obj['encodings']\n",
    "#         obj['lisa_vector'] = vector\n",
    "#         print('tagged file')\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing object {obj['documentID']}: {e}\")\n",
    "#     end_time = time.time()  # End timing\n",
    "#     elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "#     print(f\"Time taken to tag document {obj['documentID']}: {elapsed_time:.4f} seconds\")  # Print the elapsed time\n",
    "#     return obj\n",
    "\n",
    "# def tag_partition(input_file, output_file):\n",
    "#     print(f\"Tagging file {input_file} with worker {os.getpid()}\")\n",
    "#     tagged_objects = []\n",
    "\n",
    "#     try:\n",
    "#         with open(input_file, 'r') as reader:\n",
    "#             for line in reader:\n",
    "#                 obj = json.loads(line.strip())\n",
    "#                 tagged_object = tag_lisa(obj)\n",
    "#                 tagged_objects.append(tagged_object)\n",
    "# #                 print(f\"Tagged object {obj['documentID']} for worker {os.getpid()}. Count: {len(tagged_objects)}\")\n",
    "\n",
    "#                 if len(tagged_objects) % 10 == 0:\n",
    "# #                     print(f\"Appending chunk of 10 objects to {output_file}\")\n",
    "#                     append_chunk(output_file, tagged_objects)\n",
    "#                     tagged_objects = []\n",
    "\n",
    "#         if tagged_objects:\n",
    "#             append_chunk(output_file, tagged_objects)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error tagging partition {input_file}: {e}\")\n",
    "\n",
    "# def append_chunk(output_file, chunk):\n",
    "#     try:\n",
    "#         with open(output_file, 'a') as writer:\n",
    "#             for obj in chunk:\n",
    "#                 writer.write(json.dumps(obj) + '\\n')\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error appending chunk to {output_file}: {e}\")\n",
    "        \n",
    "# tag_partition('/shared/3/projects/hiatus/tagged_data/lisa_partitions/blogcorpus/corpus/partition-1.jsonl', 'test_tagged.jsonl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag each partition with 1 CPU \n",
    "\n",
    "Set nice value low so we don't hog the server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import logging\n",
    "original_level = logging.getLogger().getEffectiveLevel()\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "tag_partitions(\n",
    "               input_directory=partition_dir,\n",
    "               output_directory=tag_dir,\n",
    "               num_workers=2,\n",
    "               )\n",
    "logging.getLogger().setLevel(original_level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the partitioned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_tagged_files(input_directory=tag_dir,\n",
    "#                   output_file=os.path.join(output_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the temp directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_partitioned_files(partition_dir)\n",
    "# delete_partitioned_files(tag_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
