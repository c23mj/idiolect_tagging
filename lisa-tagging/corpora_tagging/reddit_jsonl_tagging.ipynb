{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from data_partitioner_jsonl import *\n",
    "from lisa_tagging_reddit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = 'corpus.jsonl'\n",
    "dataset = \"long-reddit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n",
      "/shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/merged\n",
      "/shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-long-reddit/corpus\n",
      "/shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-long-reddit\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# input_path = os.path.join(f'/shared/3/projects/hiatus/idiolect/data/full_pilot/{dataset}', file_name)\n",
    "# partition_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/{dataset}', file_name.split('.')[0]) \n",
    "# tag_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-{dataset}', file_name.split('.')[0])\n",
    "# output_dir= f'/shared/3/projects/hiatus/tagged_data/lisa-{dataset}'\n",
    "input_path = os.path.join(f'/shared/3/projects/hiatus/idiolect/data/full_pilot/{dataset}', file_name)\n",
    "partition_dir = os.path.join(os.path.join(f'/shared/3/projects/hiatus/tagged_data/partitions/{dataset}', file_name.split('.')[0]), 'merged') \n",
    "tag_dir = os.path.join(f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-{dataset}', file_name.split('.')[0])\n",
    "output_dir= f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/tagged-{dataset}'\n",
    "# ensures all directories will exist\n",
    "for directory in [partition_dir, tag_dir, output_dir]:\n",
    "    print(directory)\n",
    "    os.makedirs(directory, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the file into 50 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# partition_file(input_path, partition_dir, chunks=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm  # Import tqdm for progress bar\n",
    "# import time\n",
    "# import json\n",
    "# import os\n",
    "# import torch\n",
    "\n",
    "# from tokenization_enc_t5 import EncT5Tokenizer\n",
    "# from modeling_enc_t5 import EncT5ForSequenceClassification\n",
    "\n",
    "# tokenizer = EncT5Tokenizer.from_pretrained(\"t5-base\")\n",
    "# model = EncT5ForSequenceClassification.from_pretrained(\"/shared/3/projects/hiatus/idiolect/models/stylegenome_lisa_sfam/lisa_checkpoint\", num_labels=768, problem_type=\"regression\")\n",
    "# model.eval()\n",
    "\n",
    "# os.nice(0)\n",
    "\n",
    "# def tag_lisa(obj):\n",
    "#     start_time = time.time()  # Start timing\n",
    "#     # This will return a 768-dimensional LISA vector for the text, where each element is a score from 0-1, where the score corresponds to each concept in \"lisa_dimensions.json\".\n",
    "#     with torch.no_grad():\n",
    "#         tokenized = tokenizer(\n",
    "#             [obj['body']],\n",
    "#             truncation=True, max_length=512, padding=True, return_tensors=\"pt\"\n",
    "#         )\n",
    "#         prediction = model.forward(**tokenized)[0][0].cpu().detach().float()\n",
    "#         vector = torch.clamp(prediction, min=0.0, max=1.0).tolist()\n",
    "#     obj['lisa_vector'] = vector\n",
    "#     return obj\n",
    "\n",
    "# in_file_name = 'partition-21-30.jsonl'\n",
    "\n",
    "# input_file = f'{partition_dir}/{in_file_name}'\n",
    "# output_file = f'{output_dir}/{in_file_name}'\n",
    "\n",
    "# chunk_size = 50000\n",
    "# chunk = []\n",
    "# skip_lines = 100000  # Variable to skip n lines before beginning to tag\n",
    "\n",
    "# def append_chunk(output_file, chunk):\n",
    "#     try:\n",
    "#         with open(output_file, 'a', encoding='utf-8') as writer:\n",
    "#             for obj in chunk:\n",
    "#                 writer.write(json.dumps(obj) + '\\n')\n",
    "#         print(f\"Appended {len(chunk)} records to {output_file}.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error appending chunk to {output_file}: {e}\")\n",
    "\n",
    "# with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "#     for _ in range(skip_lines):  # Skip the first n lines\n",
    "#         next(infile)\n",
    "\n",
    "#     for line in tqdm(infile, desc=\"Processing lines\"):\n",
    "#         json_obj = json.loads(line.strip())  # Load JSON object from current line\n",
    "#         if 'bot' in json_obj['author'].lower():\n",
    "#             continue\n",
    "#         tagged_obj = tag_lisa(json_obj) \n",
    "#         chunk.append(tagged_obj)\n",
    "        \n",
    "#         if len(chunk) >= chunk_size:\n",
    "#             append_chunk(output_file, chunk)\n",
    "#             chunk = []\n",
    "\n",
    "#     if chunk:\n",
    "#         append_chunk(output_file, chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'EncT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'tokenization_enc_t5.EncT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Processing lines: 472it [00:55,  9.14it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from tokenization_enc_t5 import EncT5Tokenizer\n",
    "from modeling_enc_t5 import EncT5ForSequenceClassification\n",
    "\n",
    "tokenizer = EncT5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = EncT5ForSequenceClassification.from_pretrained(\"/shared/3/projects/hiatus/idiolect/models/stylegenome_lisa_sfam/lisa_checkpoint\", num_labels=768, problem_type=\"regression\")\n",
    "model.eval()\n",
    "\n",
    "os.nice(0)\n",
    "\n",
    "def tag_lisa(obj):\n",
    "    start_time = time.time()  # Start timing\n",
    "    # This will return a 768-dimensional LISA vector for the text, where each element is a score from 0-1, where the score corresponds to each concept in \"lisa_dimensions.json\".\n",
    "    with torch.no_grad():\n",
    "        tokenized = tokenizer(\n",
    "            [obj['body']],\n",
    "            truncation=True, max_length=512, padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        prediction = model.forward(**tokenized)[0][0].cpu().detach().float()\n",
    "        vector = torch.clamp(prediction, min=0.0, max=1.0).tolist()\n",
    "    obj['lisa_vector'] = vector\n",
    "    return obj\n",
    "\n",
    "in_file_name = 'partition-31-40.jsonl'\n",
    "\n",
    "input_file = f'{partition_dir}/{in_file_name}'\n",
    "output_file = f'{output_dir}/{in_file_name}'\n",
    "\n",
    "chunk_size = 50000\n",
    "chunk = []\n",
    "skip_lines = 400000  # Variable to skip n lines before beginning to tag\n",
    "\n",
    "def append_chunk(output_file, chunk):\n",
    "    try:\n",
    "        with open(output_file, 'a', encoding='utf-8') as writer:\n",
    "            for obj in chunk:\n",
    "                writer.write(json.dumps(obj) + '\\n')\n",
    "        print(f\"Appended {len(chunk)} records to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error appending chunk to {output_file}: {e}\")\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "    for _ in range(skip_lines):  # Skip the first n lines\n",
    "        next(infile)\n",
    "\n",
    "    for line in tqdm(infile, desc=\"Processing lines\"):\n",
    "        json_obj = json.loads(line.strip())  # Load JSON object from current line\n",
    "        if 'bot' in json_obj['author'].lower():\n",
    "            continue\n",
    "        tagged_obj = tag_lisa(json_obj) \n",
    "        chunk.append(tagged_obj)\n",
    "        \n",
    "        if len(chunk) >= chunk_size:\n",
    "            append_chunk(output_file, chunk)\n",
    "            chunk = []\n",
    "\n",
    "    if chunk:\n",
    "        append_chunk(output_file, chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm  # Import tqdm for progress bar\n",
    "# import time\n",
    "# import json\n",
    "# import os\n",
    "# import torch\n",
    "\n",
    "# from tokenization_enc_t5 import EncT5Tokenizer\n",
    "# from modeling_enc_t5 import EncT5ForSequenceClassification\n",
    "\n",
    "# tokenizer = EncT5Tokenizer.from_pretrained(\"t5-base\")\n",
    "# model = EncT5ForSequenceClassification.from_pretrained(\"/shared/3/projects/hiatus/idiolect/models/stylegenome_lisa_sfam/lisa_checkpoint\", num_labels=768, problem_type=\"regression\")\n",
    "# model.eval()\n",
    "\n",
    "# os.nice(0)\n",
    "\n",
    "# def tag_lisa(obj):\n",
    "# #     start_time = time.time()  # Start timing\n",
    "#     # This will return a 768-dimensional LISA vector for the text, where each element is a score from 0-1, where the score corresponds to each concept in \"lisa_dimensions.json\".\n",
    "#     with torch.no_grad():\n",
    "#         tokenized = tokenizer(\n",
    "#             [obj['body']],\n",
    "#             truncation=True, max_length=512, padding=True, return_tensors=\"pt\"\n",
    "#         )\n",
    "#         prediction = model.forward(**tokenized)[0][0].cpu().detach().float()\n",
    "#         vector = torch.clamp(prediction, min=0.0, max=1.0).tolist()\n",
    "#     end_time = time.time()  # End timing\n",
    "#     elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "#     print(f\"Time taken to tag document {obj['id']}: {elapsed_time:.4f} seconds\")\n",
    "#     obj['lisa_vector'] = vector\n",
    "#     return obj\n",
    "\n",
    "# # datasets = [\"long-reddit\"]\n",
    "# in_file_name = 'partition-1-10.jsonl'\n",
    "\n",
    "# input_file = f'{partition_dir}/{in_file_name}'\n",
    "# output_file = f'/shared/3/projects/hiatus/tagged_data/lisa_partitions/{in_file_name.split(\".\")[0]}.jsonl'\n",
    "\n",
    "# buffer_size = 100  # Number of JSON objects to buffer before writing to file\n",
    "# buffer = []\n",
    "\n",
    "# with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "# open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "#     for line in tqdm(infile, desc=\"Processing lines\"):\n",
    "#         json_obj = json.loads(line.strip())  # Load JSON object from current line\n",
    "#         mutated_obj = tag_lisa(json_obj)  # Mutate JSON object\n",
    "#         buffer.append(mutated_obj)\n",
    "        \n",
    "#         if len(buffer) >= buffer_size:\n",
    "#             outfile.write('\\n'.join(json.dumps(obj, ensure_ascii=False) for obj in buffer) + '\\n')\n",
    "#             buffer = []\n",
    "\n",
    "#     # Write any remaining objects in the buffer\n",
    "#     if buffer:\n",
    "#         outfile.write('\\n'.join(json.dumps(obj, ensure_ascii=False) for obj in buffer) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag each partition with 1 CPU \n",
    "\n",
    "Set nice value low so we don't hog the server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import logging\n",
    "# original_level = logging.getLogger().getEffectiveLevel()\n",
    "# logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# tag_partitions(\n",
    "#                input_directory=partition_dir,\n",
    "#                output_directory=tag_dir,\n",
    "#                num_workers=5,\n",
    "# #                default_niceness=0\n",
    "#                )\n",
    "# logging.getLogger().setLevel(original_level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the partitioned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join_tagged_files(input_directory=tag_dir,\n",
    "#                   output_file=os.path.join(output_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the temp directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete_partitioned_files(partition_dir)\n",
    "# delete_partitioned_files(tag_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
