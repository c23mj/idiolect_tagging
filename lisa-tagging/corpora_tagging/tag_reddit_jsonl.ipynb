{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: detected 160 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "INFO:numexpr.utils:Note: NumExpr detected 160 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "from data_partitioner_jsonl import *\n",
    "from lisa_tagging_reddit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'corpus.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join('/shared/3/projects/hiatus/idiolect/data/full_pilot/long-reddit', file_name)\n",
    "partition_dir = os.path.join('/shared/3/projects/hiatus/tagged_data/partitions/long-reddit', file_name.split('.')[0]) \n",
    "tag_dir = os.path.join('/shared/3/projects/hiatus/tagged_data/partitions/tagged-long-reddit', file_name.split('.')[0])\n",
    "output_dir= '/shared/3/projects/hiatus/tagged_data/idiolect/long-reddit'\n",
    "    \n",
    "# ensures all directories will exist\n",
    "for directory in [partition_dir, tag_dir, output_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# initialize author subreddit dictionary\n",
    "author_subreddit_counts = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the file into 100 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "partition_file(input_path, partition_dir, chunks=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag each partition with 1 CPU \n",
    "\n",
    "Set nice value low so we don't hog the server\n",
    "\n",
    "**Tagger config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'biber': True,\n",
       " 'function_words': True,\n",
       " 'binary_tags': True,\n",
       " 'function_words_list': [],\n",
       " 'token_normalization': 100,\n",
       " 'use_gpu': False,\n",
       " 'show_progress': False,\n",
       " 'n_processes': 1,\n",
       " 'processing_size': 10000,\n",
       " 'return_text': False,\n",
       " 'drop_last_batch_pct': 0.5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_config()\n",
    "config.update({'use_gpu': False, \n",
    "               'biber': True,\n",
    "               'binary_tags': True, \n",
    "               'function_words': True,\n",
    "               'token_normalization': 100})\n",
    "tokenizer = load_tokenizer(use_gpu=False)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-1.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-3.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-2.jsonl\n",
      "\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-7.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-11.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-13.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-5.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-9.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-4.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-12.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-8.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-10.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-15.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-6.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-16.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-19.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-20.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-18.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-17.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-23.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-22.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-14.jsonl\n",
      "\n",
      "\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-25.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-26.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-21.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-24.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-29.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-28.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-27.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-30.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-34.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-35.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-33.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-36.jsonl\n",
      "\n",
      "\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-37.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-38.jsonl\n",
      "\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-42.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-41.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-44.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-47.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-43.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-46.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-45.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-48.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-49.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-39.jsonl\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-40.jsonl\n",
      "\n",
      "\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-51.jsonl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-31.jsonl\n",
      "\n",
      "Tagging file /shared/3/projects/hiatus/tagged_data/partitions/long-reddit/corpus/partition-32.jsonl\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda/lib/python3.11/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mjjiang/biber-multidimensional-register-analysis/corpora_tagging/reddit_tagging_jsonl.py\", line 43, in tag_partition\n    obj = json.loads(obj_str)\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda/lib/python3.11/json/__init__.py\", line 339, in loads\n    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\nTypeError: the JSON object must be str, bytes or bytearray, not dict\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[0;32m~/biber-multidimensional-register-analysis/corpora_tagging/reddit_tagging_jsonl.py:27\u001b[0m, in \u001b[0;36mtag_partitions\u001b[0;34m(config, input_directory, output_directory, num_workers, post_counts, default_niceness)\u001b[0m\n\u001b[1;32m     24\u001b[0m     os\u001b[38;5;241m.\u001b[39mnice(default_niceness)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(num_workers, initializer\u001b[38;5;241m=\u001b[39mset_niceness) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 27\u001b[0m     results \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mstarmap(tag_partition, process_args)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m local_count \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     30\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m author, subreddit_dict \u001b[38;5;129;01min\u001b[39;00m local_count\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, starmapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not dict"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "\n",
    "# Suppress future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "tag_partitions(config,\n",
    "               input_directory=partition_dir,\n",
    "               output_directory=tag_dir,\n",
    "               num_workers=51,\n",
    "               post_counts=author_subreddit_counts,\n",
    "               default_niceness=0\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of authors: 0\n",
      "Total number of posts: 0\n",
      "Average number of posts per author: 0.00\n",
      "Total number of authors with over 5 total posts: 0\n",
      "Total number of authors with over 10 total posts: 0\n",
      "Total number of unique subreddits: 0\n"
     ]
    }
   ],
   "source": [
    "# utilities:\n",
    "\n",
    "# Counts the number of tagged entries across each partition\n",
    "# from collections import defaultdict\n",
    "# counts = defaultdict(int)\n",
    "# for i in range(1, 103):\n",
    "#     try:\n",
    "#         counts[count_lines(tag_dir + f\"/partition-{i}.jsonl-tagged.gz\")] += 1\n",
    "#     except Exception:\n",
    "#         continue\n",
    "# print(counts)\n",
    "\n",
    "def print_stats(post_counts):\n",
    "    total_posts = 0\n",
    "    authors_over_5 = 0\n",
    "    authors_over_10 = 0\n",
    "    unique_subs = set()\n",
    "\n",
    "    for author, subs in post_counts.items():\n",
    "        author_posts = sum(subs.values())\n",
    "        total_posts += author_posts\n",
    "        unique_subs.update(subs.keys())\n",
    "        \n",
    "        if author_posts > 5:\n",
    "            authors_over_5 += 1\n",
    "        if author_posts > 10:\n",
    "            authors_over_10 += 1\n",
    "\n",
    "    total_authors = len(post_counts)\n",
    "    avg_posts = float(total_posts) / total_authors if total_authors else 0.0\n",
    "\n",
    "    print(f\"Total number of authors: {total_authors}\")\n",
    "    print(f\"Total number of posts: {total_posts}\")\n",
    "    print(f\"Average number of posts per author: {avg_posts:.2f}\")\n",
    "    print(f\"Total number of authors with over 5 total posts: {authors_over_5}\")\n",
    "    print(f\"Total number of authors with over 10 total posts: {authors_over_10}\")\n",
    "    print(f\"Total number of unique subreddits: {len(unique_subs)}\")\n",
    "    \n",
    "# Individual partition\n",
    "print_stats(author_subreddit_counts) \n",
    "\n",
    "# #All partitions\n",
    "# import csv\n",
    "# import os\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def read_tsv_to_dict(file_path):\n",
    "#     # Function to read a single TSV and update the main dictionary\n",
    "#     with open(file_path, 'r', newline='') as f:\n",
    "#         reader = csv.DictReader(f, delimiter='\\t')\n",
    "#         for row in reader:\n",
    "#             author = row['author']\n",
    "#             subreddit = row['subreddit']\n",
    "#             post_count = int(row['post_count'])\n",
    "#             # If author exists in the dictionary, update their count for the subreddit\n",
    "#             if author in aggregated_data:\n",
    "#                 if subreddit in aggregated_data[author]:\n",
    "#                     aggregated_data[author][subreddit] += post_count\n",
    "#                 else:\n",
    "#                     aggregated_data[author][subreddit] = post_count\n",
    "#             else:\n",
    "#                 # If author does not exist, create new nested dictionary\n",
    "#                 aggregated_data[author] = {subreddit: post_count}\n",
    "\n",
    "# def process_directory(directory):\n",
    "#     # Function to process each file in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.endswith('.tsv'):\n",
    "#             file_path = os.path.join(directory, filename)\n",
    "#             read_tsv_to_dict(file_path)\n",
    "\n",
    "# # Main dictionary to hold all data\n",
    "# aggregated_data = defaultdict(dict)\n",
    "\n",
    "# # Path to the directory containing the TSV files\n",
    "# directory_path = '/shared/3/projects/hiatus/tagged_data/full_pilot/long-reddit'\n",
    "# process_directory(directory_path)\n",
    "\n",
    "# # Now aggregated_data contains all the combined data\n",
    "# print_stats(aggregated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 573 µs, sys: 366 µs, total: 939 µs\n",
      "Wall time: 3.09 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_tsv = output_dir + file_name.split('.')[0] + '-counts.tsv'\n",
    "# Write the post counts to a TSV file\n",
    "# write_to_tsv(author_subreddit_counts, output_tsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the partitioned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files to merge: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Merging tagged files:   0%|                                         | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def join_tagged_files(input_directory, output_file, buffer_size=1024*1024):\n",
    "    \"\"\"Join all tagged files from the input directory into one output file with buffered writing.\"\"\"\n",
    "    tagged_files = glob(os.path.join(input_directory, \"*.jsonl\"))\n",
    "\n",
    "    print(f\"Total files to merge: {len(tagged_files)}\")\n",
    "\n",
    "    with open(output_file, 'w', buffering=buffer_size) as writer:\n",
    "        for tagged_file in tqdm(tagged_files, desc=\"Merging tagged files\"):\n",
    "            with open(tagged_file, 'r') as reader:\n",
    "                for line in reader:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        writer.write(json.dumps(obj) + '\\n')\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Error decoding JSON from file: {tagged_file}\")\n",
    "\n",
    "\n",
    "join_tagged_files(input_directory=tag_dir,\n",
    "                  output_file=os.path.join(output_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the temp directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_partitioned_files(partition_dir)\n",
    "# delete_partitioned_files(tag_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
